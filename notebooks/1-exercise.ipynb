{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lesson 30 Orthogonal Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n",
      "dot product col 0,1\n",
      "0,1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy\n",
    "\n",
    "I3 = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "print(I3)\n",
    "\n",
    "print('dot product col 0,1')\n",
    "print('0,1')\n",
    "np.dot(I3[0], I3[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product of col 1,2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('dot product of col 1,2')\n",
    "np.dot(I3[1], I3[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product of col 0,2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('dot product of col 0,2')\n",
    "np.dot(I3[0], I3[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dot products are 0. all columns are orthogonal to each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2norm of col0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('L2norm of col0')\n",
    "np.linalg.norm(I3[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2norm of col1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('L2norm of col1')\n",
    "np.linalg.norm(I3[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2norm of col2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('L2norm of col2')\n",
    "np.linalg.norm(I3[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6667,  0.3333,  0.6667],\n",
       "        [-0.6667,  0.6667,  0.3333],\n",
       "        [ 0.3333,  0.6667, -0.6667]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.tensor([[2/3,1/3,2/3],[-2/3,2/3,1/3],[1/3,2/3,-2/3]])\n",
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot prodict of col 0 ,1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('dot product of col 0 ,1')\n",
    "torch.dot(K[0], K[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot prodict of col 1 ,2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('dot product of col 1 ,2')\n",
    "torch.dot(K[1], K[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product of col 0 ,2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('dot product of col 0 ,2')\n",
    "torch.dot(K[0], K[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of col 0, 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('norm of col 0')\n",
    "torch.norm(K[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of col 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('norm of col 1')\n",
    "torch.norm(K[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of col 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('norm of col 2')\n",
    "torch.norm(K[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises**:\n",
    "\n",
    "1. Use PyTorch to confirm $Xv = \\lambda v$ for the first eigenvector of $X$.\n",
    "2. Confirm $Xv = \\lambda v$ for the remaining eigenvectors of $X$ (you can use NumPy or PyTorch, whichever you prefer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.,  2.,  9.],\n",
       "        [ 5., 26., -5.],\n",
       "        [ 3.,  7., -1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[25.0, 2, 9], [5.0, 26, -5], [3.0, 7, -1]], dtype = torch.float)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.linalg_eig(\n",
       "eigenvalues=tensor([29.6762+0.j, 20.6212+0.j, -0.2974+0.j]),\n",
       "eigenvectors=tensor([[ 0.7118+0.j,  0.6502+0.j, -0.3422+0.j],\n",
       "        [ 0.6665+0.j, -0.7446+0.j,  0.2379+0.j],\n",
       "        [ 0.2217+0.j, -0.1509+0.j,  0.9090+0.j]]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L= torch.linalg.eig(X)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, V = L[0], L[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([29.6762, 20.6212, -0.2974])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L #eigenvalues\n",
    "L = L.type(torch.float)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7118,  0.6502, -0.3422],\n",
       "        [ 0.6665, -0.7446,  0.2379],\n",
       "        [ 0.2217, -0.1509,  0.9090]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V #eigenvectors\n",
    "V = V.type(torch.float)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7118, 0.6665, 0.2217])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_first =  V[:,0]\n",
    "v_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.6762)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_first = L[0]\n",
    "L_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.1223, 19.7798,  6.5792])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L_first*v_first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([21.1223, 19.7798,  6.5792])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(X,v_first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lesson 39\n",
    "\n",
    "**Exercises**:\n",
    "\n",
    "1. Use PyTorch to decompose the matrix $P$ (below) into its components $V$, $\\Lambda$, and $V^{-1}$. Confirm that $P = V \\Lambda V^{-1}$.\n",
    "2. Use PyTorch to decompose the symmetric matrix $S$ (below) into its components $Q$, $\\Lambda$, and $Q^T$. Confirm that $S = Q \\Lambda Q^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.,  2., -5.],\n",
       "        [ 3., -2.,  1.],\n",
       "        [ 5.,  7.,  4.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = torch.tensor([[25, 2, -5], [3, -2, 1], [5, 7, 4.]])\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldas, v = torch.linalg.eig(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([23.7644+0.j,  6.6684+0.j, -3.4328+0.j])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_ = torch.diag(ldas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23.7644+0.j,  0.0000+0.j,  0.0000+0.j],\n",
       "        [ 0.0000+0.j,  6.6684+0.j,  0.0000+0.j],\n",
       "        [ 0.0000+0.j,  0.0000+0.j, -3.4328+0.j]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_inv = torch.inverse(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1356+0.j,  0.0102+0.j, -0.2868+0.j],\n",
       "        [ 0.3914+0.j, -0.7198-0.j, -1.0032+0.j],\n",
       "        [ 0.0817+0.j, -1.1164-0.j,  0.2052+0.j]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.0000+0.j,  2.0000+0.j, -5.0000+0.j],\n",
       "        [ 3.0000+0.j, -2.0000+0.j,  1.0000+0.j],\n",
       "        [ 5.0000+0.j,  7.0000+0.j,  4.0000+0.j]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(v, torch.matmul(lda_, v_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Use PyTorch to decompose the symmetric matrix $S$ (below) into its components $Q$, $\\Lambda$, and $Q^T$. Confirm that $S = Q \\Lambda Q^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.,  2., -5.],\n",
       "        [ 2., -2.,  1.],\n",
       "        [-5.,  1.,  4.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = torch.tensor([[25, 2, -5], [2, -2, 1], [-5, 1, 4.]])\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldas, v = torch.linalg.eig(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26.2361+0.j,  3.2435+0.j, -2.4796+0.j])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_ = torch.diag(ldas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26.2361+0.j,  0.0000+0.j,  0.0000+0.j],\n",
       "        [ 0.0000+0.j,  3.2435+0.j,  0.0000+0.j],\n",
       "        [ 0.0000+0.j,  0.0000+0.j, -2.4796+0.j]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_t = torch.t(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.,  2., -5.],\n",
       "        [ 2., -2.,  1.],\n",
       "        [-5.,  1.,  4.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found ComplexFloat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-bbfdc9d584e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlda_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found ComplexFloat"
     ]
    }
   ],
   "source": [
    "torch.matmul(S, torch.matmul(S_t,lda_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lesson 42 Singular Value Decomposition\n",
    "**Exercise**: Using the matrix `P` from the preceding PyTorch exercises, demonstrate that these three SVD-eigendecomposition equations are true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = torch.tensor([[25,4,-5],[3,-2,1],[5,7,4.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.,  4., -5.],\n",
       "        [ 3., -2.,  1.],\n",
       "        [ 5.,  7.,  4.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,d,vt = torch.linalg.svd(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9730,  0.1974,  0.1193],\n",
       "        [-0.0894,  0.1539, -0.9840],\n",
       "        [-0.2126, -0.9682, -0.1321]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26.4718,  7.8794,  2.6752])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d = torch.diag(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26.4718,  0.0000,  0.0000],\n",
       "        [ 0.0000,  7.8794,  0.0000],\n",
       "        [ 0.0000,  0.0000,  2.6752]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9692, -0.1965,  0.1483],\n",
       "        [ 0.0706, -0.7989, -0.5972],\n",
       "        [-0.2358,  0.5684, -0.7882]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =torch.matmul(u,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25.0000,  4.0000, -5.0000],\n",
       "        [ 3.0000, -2.0000,  1.0000],\n",
       "        [ 5.0000,  7.0000,  4.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.matmul(u,new_d), vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 46 Trace Operator\n",
    "\n",
    "\n",
    "With the matrix `A_p` provided below: \n",
    "\n",
    "1. Use the PyTorch trace method to calculate the trace of `A_p`.\n",
    "2. Use the PyTorch Frobenius norm method and the trace method to demonstrate that $||A||_F = \\sqrt{\\mathrm{Tr}(AA^\\mathrm{T})}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from math import sqrt\n",
    "A_p = torch.tensor([[-1.,  2.],[ 3., -2.],[ 5.,  7.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  2.],\n",
       "        [ 3., -2.],\n",
       "        [ 5.,  7.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.trace(A_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.5917)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frobenious norm of A\n",
    "torch.linalg.matrix_norm(A_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.,  3.,  5.],\n",
       "        [ 2., -2.,  7.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_t = torch.transpose(A_p,0,1)\n",
    "A_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdt = torch.matmul(A_p, A_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.591663046625438"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(torch.trace(pdt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chapter 76 ML with autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. use torch or tf autodiff  to find the slope of y=x^2+2x+2 where x=2. use delta method to confirm your answer\n",
    "\n",
    "2.  use \"regression in pytorch\" notebook to simulate a new linear relationship between y and x, then fit the params m and b\n",
    "\n",
    "3. read about how differntial programming, wherein computer programms can be differentiated, could be common soon. see pennylane.ai for quantum ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer to question 1: ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=  torch.tensor([2.], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a**2+2*2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py:1013: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:417.)\n",
      "  return self._grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "external_grad = torch.tensor([2.])\n",
    "b.backward(gradient = external_grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4., 5., 6., 7.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate \n",
    "x =  torch.tensor([0,1,2,3,4,5,6,7.])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5904,  5.2656, 12.6435, 15.3458, 24.8301, 40.3066, 49.3377, 61.8671])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y = x**2+2*x+2+torch.normal(mean=torch.zeros(8), std=2)\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([0.5904,  5.2656, 12.6435, 15.3458, 24.8301, 40.3066, 49.3377, 61.8671])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer to question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "_ = ax.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7000], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assume y = mx+b\n",
    "# initialize model param\n",
    "# enable gradeient tracking\n",
    "\n",
    "m = torch.tensor([0.7]).requires_grad_()\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([0.2]). requires_grad_()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(x1, m1, b1):\n",
    "    return m1*x1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_plot(x1 ,y1, m1, b1):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    ax.scatter(x1,y1)\n",
    "    \n",
    "    x_min, x_max = ax.get_xlim()\n",
    "    y_min = regression(x_min, m1, b1).detach().item()\n",
    "    y_max = regression(x_max, m1, b1).detach().item()\n",
    "    \n",
    "    ax.set_xlim([x_min, x_max])\n",
    "\n",
    "    _ = ax.plot([x_min, x_max], [y_min, y_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "regression_plot(x,y,m,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6725,  3.8798,  7.0872, 10.2945, 13.5019, 16.7092, 19.9166, 23.1239],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "yhat = regression(x,m,b)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(388.7650, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate cost by comparing true y and y hat\n",
    "cost_func = torch.nn.MSELoss()\n",
    "cost = cost_func(yhat, y)\n",
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autodiff to calculate gradient of cost wrt params\n",
    "cost.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-410.4052])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-75.9970])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer for gradient descent\n",
    "optimizer = torch.optim.SGD([m,b], lr=0.01)\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.3114], requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m\n",
    "# init value:\n",
    "# tensor([0.7000], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4324], requires_grad=True)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\n",
    "# init value: \n",
    "# tensor([0.2000], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test it out \n",
    "regression_plot(x,y,m,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28.7872, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the new cost\n",
    "cost = cost_func(regression(x,m,b), y)\n",
    "cost\n",
    "# previos cost = tensor(388.7650, grad_fn=<MseLossBackward0>)\n",
    "# cost is reduced in one time gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, cost27.8, m gradient -7.07, b gradient 2.22\n",
      "epoch 1, cost27.3, m gradient -4.75, b gradient 2.67\n",
      "epoch 2, cost27.1, m gradient -3.27, b gradient 2.95\n",
      "epoch 3, cost26.9, m gradient -2.33, b gradient 3.12\n",
      "epoch 4, cost26.7, m gradient -1.74, b gradient 3.22\n",
      "epoch 5, cost26.6, m gradient -1.35, b gradient 3.28\n",
      "epoch 6, cost26.5, m gradient -1.11, b gradient 3.3\n",
      "epoch 7, cost26.4, m gradient -0.952, b gradient 3.32\n",
      "epoch 8, cost26.2, m gradient -0.851, b gradient 3.32\n",
      "epoch 9, cost26.1, m gradient -0.785, b gradient 3.31\n",
      "epoch 10, cost26, m gradient -0.742, b gradient 3.3\n",
      "epoch 11, cost25.9, m gradient -0.713, b gradient 3.28\n",
      "epoch 12, cost25.8, m gradient -0.693, b gradient 3.27\n",
      "epoch 13, cost25.7, m gradient -0.68, b gradient 3.25\n",
      "epoch 14, cost25.6, m gradient -0.669, b gradient 3.23\n",
      "epoch 15, cost25.5, m gradient -0.661, b gradient 3.22\n",
      "epoch 16, cost25.4, m gradient -0.655, b gradient 3.2\n",
      "epoch 17, cost25.2, m gradient -0.65, b gradient 3.18\n",
      "epoch 18, cost25.1, m gradient -0.645, b gradient 3.16\n",
      "epoch 19, cost25, m gradient -0.641, b gradient 3.14\n",
      "epoch 20, cost24.9, m gradient -0.636, b gradient 3.13\n",
      "epoch 21, cost24.8, m gradient -0.632, b gradient 3.11\n",
      "epoch 22, cost24.7, m gradient -0.629, b gradient 3.09\n",
      "epoch 23, cost24.6, m gradient -0.625, b gradient 3.07\n",
      "epoch 24, cost24.5, m gradient -0.621, b gradient 3.05\n",
      "epoch 25, cost24.4, m gradient -0.618, b gradient 3.04\n",
      "epoch 26, cost24.3, m gradient -0.614, b gradient 3.02\n",
      "epoch 27, cost24.2, m gradient -0.61, b gradient 3\n",
      "epoch 28, cost24.2, m gradient -0.607, b gradient 2.98\n",
      "epoch 29, cost24.1, m gradient -0.603, b gradient 2.97\n",
      "epoch 30, cost24, m gradient -0.6, b gradient 2.95\n",
      "epoch 31, cost23.9, m gradient -0.597, b gradient 2.93\n",
      "epoch 32, cost23.8, m gradient -0.593, b gradient 2.92\n",
      "epoch 33, cost23.7, m gradient -0.59, b gradient 2.9\n",
      "epoch 34, cost23.6, m gradient -0.586, b gradient 2.88\n",
      "epoch 35, cost23.5, m gradient -0.583, b gradient 2.87\n",
      "epoch 36, cost23.4, m gradient -0.58, b gradient 2.85\n",
      "epoch 37, cost23.4, m gradient -0.576, b gradient 2.83\n",
      "epoch 38, cost23.3, m gradient -0.573, b gradient 2.82\n",
      "epoch 39, cost23.2, m gradient -0.57, b gradient 2.8\n",
      "epoch 40, cost23.1, m gradient -0.566, b gradient 2.78\n",
      "epoch 41, cost23, m gradient -0.563, b gradient 2.77\n",
      "epoch 42, cost23, m gradient -0.56, b gradient 2.75\n",
      "epoch 43, cost22.9, m gradient -0.557, b gradient 2.74\n",
      "epoch 44, cost22.8, m gradient -0.553, b gradient 2.72\n",
      "epoch 45, cost22.7, m gradient -0.55, b gradient 2.71\n",
      "epoch 46, cost22.6, m gradient -0.547, b gradient 2.69\n",
      "epoch 47, cost22.6, m gradient -0.544, b gradient 2.67\n",
      "epoch 48, cost22.5, m gradient -0.541, b gradient 2.66\n",
      "epoch 49, cost22.4, m gradient -0.538, b gradient 2.64\n",
      "epoch 50, cost22.3, m gradient -0.534, b gradient 2.63\n",
      "epoch 51, cost22.3, m gradient -0.531, b gradient 2.61\n",
      "epoch 52, cost22.2, m gradient -0.528, b gradient 2.6\n",
      "epoch 53, cost22.1, m gradient -0.525, b gradient 2.58\n",
      "epoch 54, cost22.1, m gradient -0.522, b gradient 2.57\n",
      "epoch 55, cost22, m gradient -0.519, b gradient 2.55\n",
      "epoch 56, cost21.9, m gradient -0.516, b gradient 2.54\n",
      "epoch 57, cost21.9, m gradient -0.513, b gradient 2.52\n",
      "epoch 58, cost21.8, m gradient -0.51, b gradient 2.51\n",
      "epoch 59, cost21.7, m gradient -0.507, b gradient 2.49\n",
      "epoch 60, cost21.7, m gradient -0.504, b gradient 2.48\n",
      "epoch 61, cost21.6, m gradient -0.502, b gradient 2.47\n",
      "epoch 62, cost21.5, m gradient -0.499, b gradient 2.45\n",
      "epoch 63, cost21.5, m gradient -0.496, b gradient 2.44\n",
      "epoch 64, cost21.4, m gradient -0.493, b gradient 2.42\n",
      "epoch 65, cost21.4, m gradient -0.49, b gradient 2.41\n",
      "epoch 66, cost21.3, m gradient -0.487, b gradient 2.4\n",
      "epoch 67, cost21.2, m gradient -0.484, b gradient 2.38\n",
      "epoch 68, cost21.2, m gradient -0.482, b gradient 2.37\n",
      "epoch 69, cost21.1, m gradient -0.479, b gradient 2.35\n",
      "epoch 70, cost21.1, m gradient -0.476, b gradient 2.34\n",
      "epoch 71, cost21, m gradient -0.473, b gradient 2.33\n",
      "epoch 72, cost20.9, m gradient -0.471, b gradient 2.31\n",
      "epoch 73, cost20.9, m gradient -0.468, b gradient 2.3\n",
      "epoch 74, cost20.8, m gradient -0.465, b gradient 2.29\n",
      "epoch 75, cost20.8, m gradient -0.463, b gradient 2.27\n",
      "epoch 76, cost20.7, m gradient -0.46, b gradient 2.26\n",
      "epoch 77, cost20.7, m gradient -0.457, b gradient 2.25\n",
      "epoch 78, cost20.6, m gradient -0.455, b gradient 2.24\n",
      "epoch 79, cost20.6, m gradient -0.452, b gradient 2.22\n",
      "epoch 80, cost20.5, m gradient -0.449, b gradient 2.21\n",
      "epoch 81, cost20.5, m gradient -0.447, b gradient 2.2\n",
      "epoch 82, cost20.4, m gradient -0.444, b gradient 2.18\n",
      "epoch 83, cost20.4, m gradient -0.442, b gradient 2.17\n",
      "epoch 84, cost20.3, m gradient -0.439, b gradient 2.16\n",
      "epoch 85, cost20.3, m gradient -0.437, b gradient 2.15\n",
      "epoch 86, cost20.2, m gradient -0.434, b gradient 2.13\n",
      "epoch 87, cost20.2, m gradient -0.432, b gradient 2.12\n",
      "epoch 88, cost20.1, m gradient -0.429, b gradient 2.11\n",
      "epoch 89, cost20.1, m gradient -0.427, b gradient 2.1\n",
      "epoch 90, cost20, m gradient -0.424, b gradient 2.09\n",
      "epoch 91, cost20, m gradient -0.422, b gradient 2.07\n",
      "epoch 92, cost19.9, m gradient -0.419, b gradient 2.06\n",
      "epoch 93, cost19.9, m gradient -0.417, b gradient 2.05\n",
      "epoch 94, cost19.9, m gradient -0.414, b gradient 2.04\n",
      "epoch 95, cost19.8, m gradient -0.412, b gradient 2.03\n",
      "epoch 96, cost19.8, m gradient -0.41, b gradient 2.01\n",
      "epoch 97, cost19.7, m gradient -0.407, b gradient 2\n",
      "epoch 98, cost19.7, m gradient -0.405, b gradient 1.99\n",
      "epoch 99, cost19.6, m gradient -0.403, b gradient 1.98\n",
      "epoch 100, cost19.6, m gradient -0.4, b gradient 1.97\n",
      "epoch 101, cost19.6, m gradient -0.398, b gradient 1.96\n",
      "epoch 102, cost19.5, m gradient -0.396, b gradient 1.95\n",
      "epoch 103, cost19.5, m gradient -0.393, b gradient 1.93\n",
      "epoch 104, cost19.4, m gradient -0.391, b gradient 1.92\n",
      "epoch 105, cost19.4, m gradient -0.389, b gradient 1.91\n",
      "epoch 106, cost19.4, m gradient -0.387, b gradient 1.9\n",
      "epoch 107, cost19.3, m gradient -0.384, b gradient 1.89\n",
      "epoch 108, cost19.3, m gradient -0.382, b gradient 1.88\n",
      "epoch 109, cost19.3, m gradient -0.38, b gradient 1.87\n",
      "epoch 110, cost19.2, m gradient -0.378, b gradient 1.86\n",
      "epoch 111, cost19.2, m gradient -0.376, b gradient 1.85\n",
      "epoch 112, cost19.2, m gradient -0.373, b gradient 1.84\n",
      "epoch 113, cost19.1, m gradient -0.371, b gradient 1.83\n",
      "epoch 114, cost19.1, m gradient -0.369, b gradient 1.82\n",
      "epoch 115, cost19.1, m gradient -0.367, b gradient 1.8\n",
      "epoch 116, cost19, m gradient -0.365, b gradient 1.79\n",
      "epoch 117, cost19, m gradient -0.363, b gradient 1.78\n",
      "epoch 118, cost19, m gradient -0.361, b gradient 1.77\n",
      "epoch 119, cost18.9, m gradient -0.359, b gradient 1.76\n",
      "epoch 120, cost18.9, m gradient -0.357, b gradient 1.75\n",
      "epoch 121, cost18.9, m gradient -0.354, b gradient 1.74\n",
      "epoch 122, cost18.8, m gradient -0.352, b gradient 1.73\n",
      "epoch 123, cost18.8, m gradient -0.35, b gradient 1.72\n",
      "epoch 124, cost18.8, m gradient -0.348, b gradient 1.71\n",
      "epoch 125, cost18.7, m gradient -0.346, b gradient 1.7\n",
      "epoch 126, cost18.7, m gradient -0.344, b gradient 1.69\n",
      "epoch 127, cost18.7, m gradient -0.342, b gradient 1.68\n",
      "epoch 128, cost18.6, m gradient -0.34, b gradient 1.67\n",
      "epoch 129, cost18.6, m gradient -0.338, b gradient 1.66\n",
      "epoch 130, cost18.6, m gradient -0.337, b gradient 1.65\n",
      "epoch 131, cost18.6, m gradient -0.335, b gradient 1.65\n",
      "epoch 132, cost18.5, m gradient -0.333, b gradient 1.64\n",
      "epoch 133, cost18.5, m gradient -0.331, b gradient 1.63\n",
      "epoch 134, cost18.5, m gradient -0.329, b gradient 1.62\n",
      "epoch 135, cost18.4, m gradient -0.327, b gradient 1.61\n",
      "epoch 136, cost18.4, m gradient -0.325, b gradient 1.6\n",
      "epoch 137, cost18.4, m gradient -0.323, b gradient 1.59\n",
      "epoch 138, cost18.4, m gradient -0.321, b gradient 1.58\n",
      "epoch 139, cost18.3, m gradient -0.319, b gradient 1.57\n",
      "epoch 140, cost18.3, m gradient -0.318, b gradient 1.56\n",
      "epoch 141, cost18.3, m gradient -0.316, b gradient 1.55\n",
      "epoch 142, cost18.3, m gradient -0.314, b gradient 1.54\n",
      "epoch 143, cost18.2, m gradient -0.312, b gradient 1.54\n",
      "epoch 144, cost18.2, m gradient -0.31, b gradient 1.53\n",
      "epoch 145, cost18.2, m gradient -0.309, b gradient 1.52\n",
      "epoch 146, cost18.2, m gradient -0.307, b gradient 1.51\n",
      "epoch 147, cost18.1, m gradient -0.305, b gradient 1.5\n",
      "epoch 148, cost18.1, m gradient -0.303, b gradient 1.49\n",
      "epoch 149, cost18.1, m gradient -0.302, b gradient 1.48\n",
      "epoch 150, cost18.1, m gradient -0.3, b gradient 1.47\n",
      "epoch 151, cost18, m gradient -0.298, b gradient 1.47\n",
      "epoch 152, cost18, m gradient -0.296, b gradient 1.46\n",
      "epoch 153, cost18, m gradient -0.295, b gradient 1.45\n",
      "epoch 154, cost18, m gradient -0.293, b gradient 1.44\n",
      "epoch 155, cost18, m gradient -0.291, b gradient 1.43\n",
      "epoch 156, cost17.9, m gradient -0.29, b gradient 1.42\n",
      "epoch 157, cost17.9, m gradient -0.288, b gradient 1.42\n",
      "epoch 158, cost17.9, m gradient -0.286, b gradient 1.41\n",
      "epoch 159, cost17.9, m gradient -0.285, b gradient 1.4\n",
      "epoch 160, cost17.9, m gradient -0.283, b gradient 1.39\n",
      "epoch 161, cost17.8, m gradient -0.281, b gradient 1.38\n",
      "epoch 162, cost17.8, m gradient -0.28, b gradient 1.38\n",
      "epoch 163, cost17.8, m gradient -0.278, b gradient 1.37\n",
      "epoch 164, cost17.8, m gradient -0.276, b gradient 1.36\n",
      "epoch 165, cost17.8, m gradient -0.275, b gradient 1.35\n",
      "epoch 166, cost17.7, m gradient -0.273, b gradient 1.34\n",
      "epoch 167, cost17.7, m gradient -0.272, b gradient 1.34\n",
      "epoch 168, cost17.7, m gradient -0.27, b gradient 1.33\n",
      "epoch 169, cost17.7, m gradient -0.269, b gradient 1.32\n",
      "epoch 170, cost17.7, m gradient -0.267, b gradient 1.31\n",
      "epoch 171, cost17.6, m gradient -0.265, b gradient 1.31\n",
      "epoch 172, cost17.6, m gradient -0.264, b gradient 1.3\n",
      "epoch 173, cost17.6, m gradient -0.262, b gradient 1.29\n",
      "epoch 174, cost17.6, m gradient -0.261, b gradient 1.28\n",
      "epoch 175, cost17.6, m gradient -0.259, b gradient 1.28\n",
      "epoch 176, cost17.6, m gradient -0.258, b gradient 1.27\n",
      "epoch 177, cost17.5, m gradient -0.256, b gradient 1.26\n",
      "epoch 178, cost17.5, m gradient -0.255, b gradient 1.25\n",
      "epoch 179, cost17.5, m gradient -0.253, b gradient 1.25\n",
      "epoch 180, cost17.5, m gradient -0.252, b gradient 1.24\n",
      "epoch 181, cost17.5, m gradient -0.251, b gradient 1.23\n",
      "epoch 182, cost17.5, m gradient -0.249, b gradient 1.23\n",
      "epoch 183, cost17.4, m gradient -0.248, b gradient 1.22\n",
      "epoch 184, cost17.4, m gradient -0.246, b gradient 1.21\n",
      "epoch 185, cost17.4, m gradient -0.245, b gradient 1.2\n",
      "epoch 186, cost17.4, m gradient -0.243, b gradient 1.2\n",
      "epoch 187, cost17.4, m gradient -0.242, b gradient 1.19\n",
      "epoch 188, cost17.4, m gradient -0.241, b gradient 1.18\n",
      "epoch 189, cost17.4, m gradient -0.239, b gradient 1.18\n",
      "epoch 190, cost17.3, m gradient -0.238, b gradient 1.17\n",
      "epoch 191, cost17.3, m gradient -0.237, b gradient 1.16\n",
      "epoch 192, cost17.3, m gradient -0.235, b gradient 1.16\n",
      "epoch 193, cost17.3, m gradient -0.234, b gradient 1.15\n",
      "epoch 194, cost17.3, m gradient -0.232, b gradient 1.14\n",
      "epoch 195, cost17.3, m gradient -0.231, b gradient 1.14\n",
      "epoch 196, cost17.3, m gradient -0.23, b gradient 1.13\n",
      "epoch 197, cost17.2, m gradient -0.228, b gradient 1.12\n",
      "epoch 198, cost17.2, m gradient -0.227, b gradient 1.12\n",
      "epoch 199, cost17.2, m gradient -0.226, b gradient 1.11\n",
      "epoch 200, cost17.2, m gradient -0.225, b gradient 1.1\n",
      "epoch 201, cost17.2, m gradient -0.223, b gradient 1.1\n",
      "epoch 202, cost17.2, m gradient -0.222, b gradient 1.09\n",
      "epoch 203, cost17.2, m gradient -0.221, b gradient 1.09\n",
      "epoch 204, cost17.2, m gradient -0.219, b gradient 1.08\n",
      "epoch 205, cost17.1, m gradient -0.218, b gradient 1.07\n",
      "epoch 206, cost17.1, m gradient -0.217, b gradient 1.07\n",
      "epoch 207, cost17.1, m gradient -0.216, b gradient 1.06\n",
      "epoch 208, cost17.1, m gradient -0.214, b gradient 1.05\n",
      "epoch 209, cost17.1, m gradient -0.213, b gradient 1.05\n",
      "epoch 210, cost17.1, m gradient -0.212, b gradient 1.04\n",
      "epoch 211, cost17.1, m gradient -0.211, b gradient 1.04\n",
      "epoch 212, cost17.1, m gradient -0.209, b gradient 1.03\n",
      "epoch 213, cost17.1, m gradient -0.208, b gradient 1.02\n",
      "epoch 214, cost17, m gradient -0.207, b gradient 1.02\n",
      "epoch 215, cost17, m gradient -0.206, b gradient 1.01\n",
      "epoch 216, cost17, m gradient -0.205, b gradient 1.01\n",
      "epoch 217, cost17, m gradient -0.204, b gradient 1\n",
      "epoch 218, cost17, m gradient -0.202, b gradient 0.995\n",
      "epoch 219, cost17, m gradient -0.201, b gradient 0.989\n",
      "epoch 220, cost17, m gradient -0.2, b gradient 0.983\n",
      "epoch 221, cost17, m gradient -0.199, b gradient 0.978\n",
      "epoch 222, cost17, m gradient -0.198, b gradient 0.972\n",
      "epoch 223, cost17, m gradient -0.197, b gradient 0.967\n",
      "epoch 224, cost16.9, m gradient -0.195, b gradient 0.961\n",
      "epoch 225, cost16.9, m gradient -0.194, b gradient 0.955\n",
      "epoch 226, cost16.9, m gradient -0.193, b gradient 0.95\n",
      "epoch 227, cost16.9, m gradient -0.192, b gradient 0.944\n",
      "epoch 228, cost16.9, m gradient -0.191, b gradient 0.939\n",
      "epoch 229, cost16.9, m gradient -0.19, b gradient 0.934\n",
      "epoch 230, cost16.9, m gradient -0.189, b gradient 0.928\n",
      "epoch 231, cost16.9, m gradient -0.188, b gradient 0.923\n",
      "epoch 232, cost16.9, m gradient -0.187, b gradient 0.918\n",
      "epoch 233, cost16.9, m gradient -0.186, b gradient 0.912\n",
      "epoch 234, cost16.9, m gradient -0.184, b gradient 0.907\n",
      "epoch 235, cost16.8, m gradient -0.183, b gradient 0.902\n",
      "epoch 236, cost16.8, m gradient -0.182, b gradient 0.897\n",
      "epoch 237, cost16.8, m gradient -0.181, b gradient 0.891\n",
      "epoch 238, cost16.8, m gradient -0.18, b gradient 0.886\n",
      "epoch 239, cost16.8, m gradient -0.179, b gradient 0.881\n",
      "epoch 240, cost16.8, m gradient -0.178, b gradient 0.876\n",
      "epoch 241, cost16.8, m gradient -0.177, b gradient 0.871\n",
      "epoch 242, cost16.8, m gradient -0.176, b gradient 0.866\n",
      "epoch 243, cost16.8, m gradient -0.175, b gradient 0.861\n",
      "epoch 244, cost16.8, m gradient -0.174, b gradient 0.856\n",
      "epoch 245, cost16.8, m gradient -0.173, b gradient 0.851\n",
      "epoch 246, cost16.8, m gradient -0.172, b gradient 0.846\n",
      "epoch 247, cost16.7, m gradient -0.171, b gradient 0.841\n",
      "epoch 248, cost16.7, m gradient -0.17, b gradient 0.836\n",
      "epoch 249, cost16.7, m gradient -0.169, b gradient 0.832\n",
      "epoch 250, cost16.7, m gradient -0.168, b gradient 0.827\n",
      "epoch 251, cost16.7, m gradient -0.167, b gradient 0.822\n",
      "epoch 252, cost16.7, m gradient -0.166, b gradient 0.817\n",
      "epoch 253, cost16.7, m gradient -0.165, b gradient 0.813\n",
      "epoch 254, cost16.7, m gradient -0.164, b gradient 0.808\n",
      "epoch 255, cost16.7, m gradient -0.163, b gradient 0.803\n",
      "epoch 256, cost16.7, m gradient -0.162, b gradient 0.799\n",
      "epoch 257, cost16.7, m gradient -0.161, b gradient 0.794\n",
      "epoch 258, cost16.7, m gradient -0.161, b gradient 0.789\n",
      "epoch 259, cost16.7, m gradient -0.16, b gradient 0.785\n",
      "epoch 260, cost16.7, m gradient -0.159, b gradient 0.78\n",
      "epoch 261, cost16.7, m gradient -0.158, b gradient 0.776\n",
      "epoch 262, cost16.6, m gradient -0.157, b gradient 0.771\n",
      "epoch 263, cost16.6, m gradient -0.156, b gradient 0.767\n",
      "epoch 264, cost16.6, m gradient -0.155, b gradient 0.763\n",
      "epoch 265, cost16.6, m gradient -0.154, b gradient 0.758\n",
      "epoch 266, cost16.6, m gradient -0.153, b gradient 0.754\n",
      "epoch 267, cost16.6, m gradient -0.152, b gradient 0.749\n",
      "epoch 268, cost16.6, m gradient -0.152, b gradient 0.745\n",
      "epoch 269, cost16.6, m gradient -0.151, b gradient 0.741\n",
      "epoch 270, cost16.6, m gradient -0.15, b gradient 0.737\n",
      "epoch 271, cost16.6, m gradient -0.149, b gradient 0.732\n",
      "epoch 272, cost16.6, m gradient -0.148, b gradient 0.728\n",
      "epoch 273, cost16.6, m gradient -0.147, b gradient 0.724\n",
      "epoch 274, cost16.6, m gradient -0.146, b gradient 0.72\n",
      "epoch 275, cost16.6, m gradient -0.145, b gradient 0.716\n",
      "epoch 276, cost16.6, m gradient -0.145, b gradient 0.711\n",
      "epoch 277, cost16.6, m gradient -0.144, b gradient 0.707\n",
      "epoch 278, cost16.6, m gradient -0.143, b gradient 0.703\n",
      "epoch 279, cost16.6, m gradient -0.142, b gradient 0.699\n",
      "epoch 280, cost16.5, m gradient -0.141, b gradient 0.695\n",
      "epoch 281, cost16.5, m gradient -0.141, b gradient 0.691\n",
      "epoch 282, cost16.5, m gradient -0.14, b gradient 0.687\n",
      "epoch 283, cost16.5, m gradient -0.139, b gradient 0.683\n",
      "epoch 284, cost16.5, m gradient -0.138, b gradient 0.679\n",
      "epoch 285, cost16.5, m gradient -0.137, b gradient 0.675\n",
      "epoch 286, cost16.5, m gradient -0.137, b gradient 0.671\n",
      "epoch 287, cost16.5, m gradient -0.136, b gradient 0.668\n",
      "epoch 288, cost16.5, m gradient -0.135, b gradient 0.664\n",
      "epoch 289, cost16.5, m gradient -0.134, b gradient 0.66\n",
      "epoch 290, cost16.5, m gradient -0.133, b gradient 0.656\n",
      "epoch 291, cost16.5, m gradient -0.133, b gradient 0.652\n",
      "epoch 292, cost16.5, m gradient -0.132, b gradient 0.649\n",
      "epoch 293, cost16.5, m gradient -0.131, b gradient 0.645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 294, cost16.5, m gradient -0.13, b gradient 0.641\n",
      "epoch 295, cost16.5, m gradient -0.13, b gradient 0.637\n",
      "epoch 296, cost16.5, m gradient -0.129, b gradient 0.634\n",
      "epoch 297, cost16.5, m gradient -0.128, b gradient 0.63\n",
      "epoch 298, cost16.5, m gradient -0.127, b gradient 0.626\n",
      "epoch 299, cost16.5, m gradient -0.127, b gradient 0.623\n",
      "epoch 300, cost16.5, m gradient -0.126, b gradient 0.619\n",
      "epoch 301, cost16.5, m gradient -0.125, b gradient 0.616\n",
      "epoch 302, cost16.4, m gradient -0.124, b gradient 0.612\n",
      "epoch 303, cost16.4, m gradient -0.124, b gradient 0.609\n",
      "epoch 304, cost16.4, m gradient -0.123, b gradient 0.605\n",
      "epoch 305, cost16.4, m gradient -0.122, b gradient 0.602\n",
      "epoch 306, cost16.4, m gradient -0.122, b gradient 0.598\n",
      "epoch 307, cost16.4, m gradient -0.121, b gradient 0.595\n",
      "epoch 308, cost16.4, m gradient -0.12, b gradient 0.591\n",
      "epoch 309, cost16.4, m gradient -0.12, b gradient 0.588\n",
      "epoch 310, cost16.4, m gradient -0.119, b gradient 0.584\n",
      "epoch 311, cost16.4, m gradient -0.118, b gradient 0.581\n",
      "epoch 312, cost16.4, m gradient -0.118, b gradient 0.578\n",
      "epoch 313, cost16.4, m gradient -0.117, b gradient 0.574\n",
      "epoch 314, cost16.4, m gradient -0.116, b gradient 0.571\n",
      "epoch 315, cost16.4, m gradient -0.115, b gradient 0.568\n",
      "epoch 316, cost16.4, m gradient -0.115, b gradient 0.565\n",
      "epoch 317, cost16.4, m gradient -0.114, b gradient 0.561\n",
      "epoch 318, cost16.4, m gradient -0.113, b gradient 0.558\n",
      "epoch 319, cost16.4, m gradient -0.113, b gradient 0.555\n",
      "epoch 320, cost16.4, m gradient -0.112, b gradient 0.552\n",
      "epoch 321, cost16.4, m gradient -0.112, b gradient 0.548\n",
      "epoch 322, cost16.4, m gradient -0.111, b gradient 0.545\n",
      "epoch 323, cost16.4, m gradient -0.11, b gradient 0.542\n",
      "epoch 324, cost16.4, m gradient -0.11, b gradient 0.539\n",
      "epoch 325, cost16.4, m gradient -0.109, b gradient 0.536\n",
      "epoch 326, cost16.4, m gradient -0.108, b gradient 0.533\n",
      "epoch 327, cost16.4, m gradient -0.108, b gradient 0.53\n",
      "epoch 328, cost16.4, m gradient -0.107, b gradient 0.527\n",
      "epoch 329, cost16.4, m gradient -0.107, b gradient 0.524\n",
      "epoch 330, cost16.4, m gradient -0.106, b gradient 0.521\n",
      "epoch 331, cost16.4, m gradient -0.105, b gradient 0.518\n",
      "epoch 332, cost16.3, m gradient -0.105, b gradient 0.515\n",
      "epoch 333, cost16.3, m gradient -0.104, b gradient 0.512\n",
      "epoch 334, cost16.3, m gradient -0.103, b gradient 0.509\n",
      "epoch 335, cost16.3, m gradient -0.103, b gradient 0.506\n",
      "epoch 336, cost16.3, m gradient -0.102, b gradient 0.503\n",
      "epoch 337, cost16.3, m gradient -0.102, b gradient 0.5\n",
      "epoch 338, cost16.3, m gradient -0.101, b gradient 0.497\n",
      "epoch 339, cost16.3, m gradient -0.101, b gradient 0.494\n",
      "epoch 340, cost16.3, m gradient -0.0999, b gradient 0.491\n",
      "epoch 341, cost16.3, m gradient -0.0993, b gradient 0.489\n",
      "epoch 342, cost16.3, m gradient -0.0988, b gradient 0.486\n",
      "epoch 343, cost16.3, m gradient -0.0982, b gradient 0.483\n",
      "epoch 344, cost16.3, m gradient -0.0976, b gradient 0.48\n",
      "epoch 345, cost16.3, m gradient -0.097, b gradient 0.477\n",
      "epoch 346, cost16.3, m gradient -0.0965, b gradient 0.475\n",
      "epoch 347, cost16.3, m gradient -0.0959, b gradient 0.472\n",
      "epoch 348, cost16.3, m gradient -0.0954, b gradient 0.469\n",
      "epoch 349, cost16.3, m gradient -0.0948, b gradient 0.466\n",
      "epoch 350, cost16.3, m gradient -0.0943, b gradient 0.464\n",
      "epoch 351, cost16.3, m gradient -0.0938, b gradient 0.461\n",
      "epoch 352, cost16.3, m gradient -0.0933, b gradient 0.458\n",
      "epoch 353, cost16.3, m gradient -0.0927, b gradient 0.456\n",
      "epoch 354, cost16.3, m gradient -0.0921, b gradient 0.453\n",
      "epoch 355, cost16.3, m gradient -0.0916, b gradient 0.451\n",
      "epoch 356, cost16.3, m gradient -0.0911, b gradient 0.448\n",
      "epoch 357, cost16.3, m gradient -0.0906, b gradient 0.445\n",
      "epoch 358, cost16.3, m gradient -0.09, b gradient 0.443\n",
      "epoch 359, cost16.3, m gradient -0.0895, b gradient 0.44\n",
      "epoch 360, cost16.3, m gradient -0.089, b gradient 0.438\n",
      "epoch 361, cost16.3, m gradient -0.0885, b gradient 0.435\n",
      "epoch 362, cost16.3, m gradient -0.088, b gradient 0.433\n",
      "epoch 363, cost16.3, m gradient -0.0875, b gradient 0.43\n",
      "epoch 364, cost16.3, m gradient -0.087, b gradient 0.428\n",
      "epoch 365, cost16.3, m gradient -0.0865, b gradient 0.425\n",
      "epoch 366, cost16.3, m gradient -0.086, b gradient 0.423\n",
      "epoch 367, cost16.3, m gradient -0.0855, b gradient 0.42\n",
      "epoch 368, cost16.3, m gradient -0.085, b gradient 0.418\n",
      "epoch 369, cost16.3, m gradient -0.0845, b gradient 0.416\n",
      "epoch 370, cost16.3, m gradient -0.084, b gradient 0.413\n",
      "epoch 371, cost16.3, m gradient -0.0835, b gradient 0.411\n",
      "epoch 372, cost16.3, m gradient -0.0831, b gradient 0.408\n",
      "epoch 373, cost16.3, m gradient -0.0826, b gradient 0.406\n",
      "epoch 374, cost16.3, m gradient -0.0821, b gradient 0.404\n",
      "epoch 375, cost16.3, m gradient -0.0816, b gradient 0.401\n",
      "epoch 376, cost16.3, m gradient -0.0811, b gradient 0.399\n",
      "epoch 377, cost16.3, m gradient -0.0807, b gradient 0.397\n",
      "epoch 378, cost16.2, m gradient -0.0802, b gradient 0.394\n",
      "epoch 379, cost16.2, m gradient -0.0797, b gradient 0.392\n",
      "epoch 380, cost16.2, m gradient -0.0793, b gradient 0.39\n",
      "epoch 381, cost16.2, m gradient -0.0788, b gradient 0.388\n",
      "epoch 382, cost16.2, m gradient -0.0784, b gradient 0.385\n",
      "epoch 383, cost16.2, m gradient -0.0779, b gradient 0.383\n",
      "epoch 384, cost16.2, m gradient -0.0775, b gradient 0.381\n",
      "epoch 385, cost16.2, m gradient -0.077, b gradient 0.379\n",
      "epoch 386, cost16.2, m gradient -0.0766, b gradient 0.377\n",
      "epoch 387, cost16.2, m gradient -0.0761, b gradient 0.374\n",
      "epoch 388, cost16.2, m gradient -0.0757, b gradient 0.372\n",
      "epoch 389, cost16.2, m gradient -0.0753, b gradient 0.37\n",
      "epoch 390, cost16.2, m gradient -0.0748, b gradient 0.368\n",
      "epoch 391, cost16.2, m gradient -0.0744, b gradient 0.366\n",
      "epoch 392, cost16.2, m gradient -0.074, b gradient 0.364\n",
      "epoch 393, cost16.2, m gradient -0.0735, b gradient 0.362\n",
      "epoch 394, cost16.2, m gradient -0.0731, b gradient 0.36\n",
      "epoch 395, cost16.2, m gradient -0.0727, b gradient 0.358\n",
      "epoch 396, cost16.2, m gradient -0.0723, b gradient 0.355\n",
      "epoch 397, cost16.2, m gradient -0.0719, b gradient 0.353\n",
      "epoch 398, cost16.2, m gradient -0.0714, b gradient 0.351\n",
      "epoch 399, cost16.2, m gradient -0.071, b gradient 0.349\n",
      "epoch 400, cost16.2, m gradient -0.0706, b gradient 0.347\n",
      "epoch 401, cost16.2, m gradient -0.0702, b gradient 0.345\n",
      "epoch 402, cost16.2, m gradient -0.0698, b gradient 0.343\n",
      "epoch 403, cost16.2, m gradient -0.0694, b gradient 0.341\n",
      "epoch 404, cost16.2, m gradient -0.069, b gradient 0.339\n",
      "epoch 405, cost16.2, m gradient -0.0686, b gradient 0.337\n",
      "epoch 406, cost16.2, m gradient -0.0682, b gradient 0.335\n",
      "epoch 407, cost16.2, m gradient -0.0678, b gradient 0.334\n",
      "epoch 408, cost16.2, m gradient -0.0674, b gradient 0.332\n",
      "epoch 409, cost16.2, m gradient -0.0671, b gradient 0.33\n",
      "epoch 410, cost16.2, m gradient -0.0667, b gradient 0.328\n",
      "epoch 411, cost16.2, m gradient -0.0663, b gradient 0.326\n",
      "epoch 412, cost16.2, m gradient -0.0659, b gradient 0.324\n",
      "epoch 413, cost16.2, m gradient -0.0655, b gradient 0.322\n",
      "epoch 414, cost16.2, m gradient -0.0652, b gradient 0.32\n",
      "epoch 415, cost16.2, m gradient -0.0648, b gradient 0.318\n",
      "epoch 416, cost16.2, m gradient -0.0644, b gradient 0.317\n",
      "epoch 417, cost16.2, m gradient -0.064, b gradient 0.315\n",
      "epoch 418, cost16.2, m gradient -0.0637, b gradient 0.313\n",
      "epoch 419, cost16.2, m gradient -0.0633, b gradient 0.311\n",
      "epoch 420, cost16.2, m gradient -0.0629, b gradient 0.309\n",
      "epoch 421, cost16.2, m gradient -0.0626, b gradient 0.308\n",
      "epoch 422, cost16.2, m gradient -0.0622, b gradient 0.306\n",
      "epoch 423, cost16.2, m gradient -0.0618, b gradient 0.304\n",
      "epoch 424, cost16.2, m gradient -0.0615, b gradient 0.302\n",
      "epoch 425, cost16.2, m gradient -0.0611, b gradient 0.301\n",
      "epoch 426, cost16.2, m gradient -0.0608, b gradient 0.299\n",
      "epoch 427, cost16.2, m gradient -0.0604, b gradient 0.297\n",
      "epoch 428, cost16.2, m gradient -0.0601, b gradient 0.295\n",
      "epoch 429, cost16.2, m gradient -0.0597, b gradient 0.294\n",
      "epoch 430, cost16.2, m gradient -0.0594, b gradient 0.292\n",
      "epoch 431, cost16.2, m gradient -0.059, b gradient 0.29\n",
      "epoch 432, cost16.2, m gradient -0.0587, b gradient 0.289\n",
      "epoch 433, cost16.2, m gradient -0.0584, b gradient 0.287\n",
      "epoch 434, cost16.2, m gradient -0.058, b gradient 0.285\n",
      "epoch 435, cost16.2, m gradient -0.0577, b gradient 0.284\n",
      "epoch 436, cost16.2, m gradient -0.0573, b gradient 0.282\n",
      "epoch 437, cost16.2, m gradient -0.057, b gradient 0.28\n",
      "epoch 438, cost16.2, m gradient -0.0567, b gradient 0.279\n",
      "epoch 439, cost16.2, m gradient -0.0564, b gradient 0.277\n",
      "epoch 440, cost16.2, m gradient -0.056, b gradient 0.276\n",
      "epoch 441, cost16.2, m gradient -0.0557, b gradient 0.274\n",
      "epoch 442, cost16.2, m gradient -0.0554, b gradient 0.272\n",
      "epoch 443, cost16.2, m gradient -0.0551, b gradient 0.271\n",
      "epoch 444, cost16.2, m gradient -0.0548, b gradient 0.269\n",
      "epoch 445, cost16.2, m gradient -0.0545, b gradient 0.268\n",
      "epoch 446, cost16.2, m gradient -0.0541, b gradient 0.266\n",
      "epoch 447, cost16.2, m gradient -0.0538, b gradient 0.265\n",
      "epoch 448, cost16.2, m gradient -0.0535, b gradient 0.263\n",
      "epoch 449, cost16.2, m gradient -0.0532, b gradient 0.262\n",
      "epoch 450, cost16.2, m gradient -0.0529, b gradient 0.26\n",
      "epoch 451, cost16.2, m gradient -0.0526, b gradient 0.259\n",
      "epoch 452, cost16.2, m gradient -0.0523, b gradient 0.257\n",
      "epoch 453, cost16.2, m gradient -0.052, b gradient 0.256\n",
      "epoch 454, cost16.2, m gradient -0.0517, b gradient 0.254\n",
      "epoch 455, cost16.2, m gradient -0.0514, b gradient 0.253\n",
      "epoch 456, cost16.2, m gradient -0.0511, b gradient 0.251\n",
      "epoch 457, cost16.2, m gradient -0.0508, b gradient 0.25\n",
      "epoch 458, cost16.2, m gradient -0.0505, b gradient 0.248\n",
      "epoch 459, cost16.2, m gradient -0.0502, b gradient 0.247\n",
      "epoch 460, cost16.2, m gradient -0.0499, b gradient 0.246\n",
      "epoch 461, cost16.2, m gradient -0.0496, b gradient 0.244\n",
      "epoch 462, cost16.2, m gradient -0.0493, b gradient 0.243\n",
      "epoch 463, cost16.2, m gradient -0.0491, b gradient 0.241\n",
      "epoch 464, cost16.2, m gradient -0.0488, b gradient 0.24\n",
      "epoch 465, cost16.2, m gradient -0.0485, b gradient 0.239\n",
      "epoch 466, cost16.2, m gradient -0.0482, b gradient 0.237\n",
      "epoch 467, cost16.2, m gradient -0.0479, b gradient 0.236\n",
      "epoch 468, cost16.2, m gradient -0.0477, b gradient 0.234\n",
      "epoch 469, cost16.2, m gradient -0.0474, b gradient 0.233\n",
      "epoch 470, cost16.2, m gradient -0.0471, b gradient 0.232\n",
      "epoch 471, cost16.2, m gradient -0.0468, b gradient 0.23\n",
      "epoch 472, cost16.2, m gradient -0.0466, b gradient 0.229\n",
      "epoch 473, cost16.2, m gradient -0.0463, b gradient 0.228\n",
      "epoch 474, cost16.2, m gradient -0.0461, b gradient 0.226\n",
      "epoch 475, cost16.2, m gradient -0.0458, b gradient 0.225\n",
      "epoch 476, cost16.2, m gradient -0.0455, b gradient 0.224\n",
      "epoch 477, cost16.2, m gradient -0.0453, b gradient 0.223\n",
      "epoch 478, cost16.2, m gradient -0.045, b gradient 0.221\n",
      "epoch 479, cost16.2, m gradient -0.0447, b gradient 0.22\n",
      "epoch 480, cost16.2, m gradient -0.0445, b gradient 0.219\n",
      "epoch 481, cost16.2, m gradient -0.0442, b gradient 0.217\n",
      "epoch 482, cost16.2, m gradient -0.0439, b gradient 0.216\n",
      "epoch 483, cost16.2, m gradient -0.0437, b gradient 0.215\n",
      "epoch 484, cost16.2, m gradient -0.0435, b gradient 0.214\n",
      "epoch 485, cost16.1, m gradient -0.0432, b gradient 0.212\n",
      "epoch 486, cost16.1, m gradient -0.043, b gradient 0.211\n",
      "epoch 487, cost16.1, m gradient -0.0427, b gradient 0.21\n",
      "epoch 488, cost16.1, m gradient -0.0425, b gradient 0.209\n",
      "epoch 489, cost16.1, m gradient -0.0422, b gradient 0.208\n",
      "epoch 490, cost16.1, m gradient -0.042, b gradient 0.206\n",
      "epoch 491, cost16.1, m gradient -0.0417, b gradient 0.205\n",
      "epoch 492, cost16.1, m gradient -0.0415, b gradient 0.204\n",
      "epoch 493, cost16.1, m gradient -0.0412, b gradient 0.203\n",
      "epoch 494, cost16.1, m gradient -0.041, b gradient 0.202\n",
      "epoch 495, cost16.1, m gradient -0.0408, b gradient 0.201\n",
      "epoch 496, cost16.1, m gradient -0.0405, b gradient 0.199\n",
      "epoch 497, cost16.1, m gradient -0.0403, b gradient 0.198\n",
      "epoch 498, cost16.1, m gradient -0.0401, b gradient 0.197\n",
      "epoch 499, cost16.1, m gradient -0.0399, b gradient 0.196\n",
      "epoch 500, cost16.1, m gradient -0.0396, b gradient 0.195\n",
      "epoch 501, cost16.1, m gradient -0.0394, b gradient 0.194\n",
      "epoch 502, cost16.1, m gradient -0.0392, b gradient 0.193\n",
      "epoch 503, cost16.1, m gradient -0.0389, b gradient 0.191\n",
      "epoch 504, cost16.1, m gradient -0.0387, b gradient 0.19\n",
      "epoch 505, cost16.1, m gradient -0.0385, b gradient 0.189\n",
      "epoch 506, cost16.1, m gradient -0.0382, b gradient 0.188\n",
      "epoch 507, cost16.1, m gradient -0.038, b gradient 0.187\n",
      "epoch 508, cost16.1, m gradient -0.0378, b gradient 0.186\n",
      "epoch 509, cost16.1, m gradient -0.0376, b gradient 0.185\n",
      "epoch 510, cost16.1, m gradient -0.0374, b gradient 0.184\n",
      "epoch 511, cost16.1, m gradient -0.0372, b gradient 0.183\n",
      "epoch 512, cost16.1, m gradient -0.0369, b gradient 0.182\n",
      "epoch 513, cost16.1, m gradient -0.0368, b gradient 0.181\n",
      "epoch 514, cost16.1, m gradient -0.0366, b gradient 0.18\n",
      "epoch 515, cost16.1, m gradient -0.0364, b gradient 0.179\n",
      "epoch 516, cost16.1, m gradient -0.0361, b gradient 0.178\n",
      "epoch 517, cost16.1, m gradient -0.0359, b gradient 0.177\n",
      "epoch 518, cost16.1, m gradient -0.0357, b gradient 0.176\n",
      "epoch 519, cost16.1, m gradient -0.0355, b gradient 0.175\n",
      "epoch 520, cost16.1, m gradient -0.0353, b gradient 0.174\n",
      "epoch 521, cost16.1, m gradient -0.0351, b gradient 0.173\n",
      "epoch 522, cost16.1, m gradient -0.0349, b gradient 0.172\n",
      "epoch 523, cost16.1, m gradient -0.0347, b gradient 0.171\n",
      "epoch 524, cost16.1, m gradient -0.0345, b gradient 0.17\n",
      "epoch 525, cost16.1, m gradient -0.0343, b gradient 0.169\n",
      "epoch 526, cost16.1, m gradient -0.0341, b gradient 0.168\n",
      "epoch 527, cost16.1, m gradient -0.0339, b gradient 0.167\n",
      "epoch 528, cost16.1, m gradient -0.0337, b gradient 0.166\n",
      "epoch 529, cost16.1, m gradient -0.0335, b gradient 0.165\n",
      "epoch 530, cost16.1, m gradient -0.0333, b gradient 0.164\n",
      "epoch 531, cost16.1, m gradient -0.0331, b gradient 0.163\n",
      "epoch 532, cost16.1, m gradient -0.0329, b gradient 0.162\n",
      "epoch 533, cost16.1, m gradient -0.0328, b gradient 0.161\n",
      "epoch 534, cost16.1, m gradient -0.0325, b gradient 0.16\n",
      "epoch 535, cost16.1, m gradient -0.0324, b gradient 0.159\n",
      "epoch 536, cost16.1, m gradient -0.0322, b gradient 0.158\n",
      "epoch 537, cost16.1, m gradient -0.032, b gradient 0.157\n",
      "epoch 538, cost16.1, m gradient -0.0318, b gradient 0.156\n",
      "epoch 539, cost16.1, m gradient -0.0316, b gradient 0.155\n",
      "epoch 540, cost16.1, m gradient -0.0314, b gradient 0.155\n",
      "epoch 541, cost16.1, m gradient -0.0312, b gradient 0.154\n",
      "epoch 542, cost16.1, m gradient -0.0311, b gradient 0.153\n",
      "epoch 543, cost16.1, m gradient -0.0309, b gradient 0.152\n",
      "epoch 544, cost16.1, m gradient -0.0307, b gradient 0.151\n",
      "epoch 545, cost16.1, m gradient -0.0305, b gradient 0.15\n",
      "epoch 546, cost16.1, m gradient -0.0304, b gradient 0.149\n",
      "epoch 547, cost16.1, m gradient -0.0302, b gradient 0.148\n",
      "epoch 548, cost16.1, m gradient -0.03, b gradient 0.148\n",
      "epoch 549, cost16.1, m gradient -0.0298, b gradient 0.147\n",
      "epoch 550, cost16.1, m gradient -0.0297, b gradient 0.146\n",
      "epoch 551, cost16.1, m gradient -0.0295, b gradient 0.145\n",
      "epoch 552, cost16.1, m gradient -0.0293, b gradient 0.144\n",
      "epoch 553, cost16.1, m gradient -0.0291, b gradient 0.143\n",
      "epoch 554, cost16.1, m gradient -0.029, b gradient 0.143\n",
      "epoch 555, cost16.1, m gradient -0.0288, b gradient 0.142\n",
      "epoch 556, cost16.1, m gradient -0.0286, b gradient 0.141\n",
      "epoch 557, cost16.1, m gradient -0.0285, b gradient 0.14\n",
      "epoch 558, cost16.1, m gradient -0.0283, b gradient 0.139\n",
      "epoch 559, cost16.1, m gradient -0.0282, b gradient 0.139\n",
      "epoch 560, cost16.1, m gradient -0.028, b gradient 0.138\n",
      "epoch 561, cost16.1, m gradient -0.0278, b gradient 0.137\n",
      "epoch 562, cost16.1, m gradient -0.0277, b gradient 0.136\n",
      "epoch 563, cost16.1, m gradient -0.0275, b gradient 0.135\n",
      "epoch 564, cost16.1, m gradient -0.0274, b gradient 0.135\n",
      "epoch 565, cost16.1, m gradient -0.0272, b gradient 0.134\n",
      "epoch 566, cost16.1, m gradient -0.027, b gradient 0.133\n",
      "epoch 567, cost16.1, m gradient -0.0269, b gradient 0.132\n",
      "epoch 568, cost16.1, m gradient -0.0268, b gradient 0.131\n",
      "epoch 569, cost16.1, m gradient -0.0266, b gradient 0.131\n",
      "epoch 570, cost16.1, m gradient -0.0264, b gradient 0.13\n",
      "epoch 571, cost16.1, m gradient -0.0263, b gradient 0.129\n",
      "epoch 572, cost16.1, m gradient -0.0261, b gradient 0.128\n",
      "epoch 573, cost16.1, m gradient -0.026, b gradient 0.128\n",
      "epoch 574, cost16.1, m gradient -0.0258, b gradient 0.127\n",
      "epoch 575, cost16.1, m gradient -0.0257, b gradient 0.126\n",
      "epoch 576, cost16.1, m gradient -0.0255, b gradient 0.126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 577, cost16.1, m gradient -0.0254, b gradient 0.125\n",
      "epoch 578, cost16.1, m gradient -0.0252, b gradient 0.124\n",
      "epoch 579, cost16.1, m gradient -0.0251, b gradient 0.123\n",
      "epoch 580, cost16.1, m gradient -0.0249, b gradient 0.123\n",
      "epoch 581, cost16.1, m gradient -0.0248, b gradient 0.122\n",
      "epoch 582, cost16.1, m gradient -0.0246, b gradient 0.121\n",
      "epoch 583, cost16.1, m gradient -0.0245, b gradient 0.121\n",
      "epoch 584, cost16.1, m gradient -0.0244, b gradient 0.12\n",
      "epoch 585, cost16.1, m gradient -0.0242, b gradient 0.119\n",
      "epoch 586, cost16.1, m gradient -0.0241, b gradient 0.118\n",
      "epoch 587, cost16.1, m gradient -0.0239, b gradient 0.118\n",
      "epoch 588, cost16.1, m gradient -0.0238, b gradient 0.117\n",
      "epoch 589, cost16.1, m gradient -0.0237, b gradient 0.116\n",
      "epoch 590, cost16.1, m gradient -0.0235, b gradient 0.116\n",
      "epoch 591, cost16.1, m gradient -0.0234, b gradient 0.115\n",
      "epoch 592, cost16.1, m gradient -0.0233, b gradient 0.114\n",
      "epoch 593, cost16.1, m gradient -0.0231, b gradient 0.114\n",
      "epoch 594, cost16.1, m gradient -0.023, b gradient 0.113\n",
      "epoch 595, cost16.1, m gradient -0.0229, b gradient 0.112\n",
      "epoch 596, cost16.1, m gradient -0.0227, b gradient 0.112\n",
      "epoch 597, cost16.1, m gradient -0.0226, b gradient 0.111\n",
      "epoch 598, cost16.1, m gradient -0.0225, b gradient 0.111\n",
      "epoch 599, cost16.1, m gradient -0.0224, b gradient 0.11\n",
      "epoch 600, cost16.1, m gradient -0.0222, b gradient 0.109\n",
      "epoch 601, cost16.1, m gradient -0.0221, b gradient 0.109\n",
      "epoch 602, cost16.1, m gradient -0.022, b gradient 0.108\n",
      "epoch 603, cost16.1, m gradient -0.0218, b gradient 0.107\n",
      "epoch 604, cost16.1, m gradient -0.0217, b gradient 0.107\n",
      "epoch 605, cost16.1, m gradient -0.0216, b gradient 0.106\n",
      "epoch 606, cost16.1, m gradient -0.0215, b gradient 0.106\n",
      "epoch 607, cost16.1, m gradient -0.0213, b gradient 0.105\n",
      "epoch 608, cost16.1, m gradient -0.0212, b gradient 0.104\n",
      "epoch 609, cost16.1, m gradient -0.0211, b gradient 0.104\n",
      "epoch 610, cost16.1, m gradient -0.021, b gradient 0.103\n",
      "epoch 611, cost16.1, m gradient -0.0209, b gradient 0.103\n",
      "epoch 612, cost16.1, m gradient -0.0207, b gradient 0.102\n",
      "epoch 613, cost16.1, m gradient -0.0206, b gradient 0.101\n",
      "epoch 614, cost16.1, m gradient -0.0205, b gradient 0.101\n",
      "epoch 615, cost16.1, m gradient -0.0204, b gradient 0.1\n",
      "epoch 616, cost16.1, m gradient -0.0203, b gradient 0.0996\n",
      "epoch 617, cost16.1, m gradient -0.0202, b gradient 0.099\n",
      "epoch 618, cost16.1, m gradient -0.0201, b gradient 0.0985\n",
      "epoch 619, cost16.1, m gradient -0.0199, b gradient 0.0979\n",
      "epoch 620, cost16.1, m gradient -0.0198, b gradient 0.0973\n",
      "epoch 621, cost16.1, m gradient -0.0197, b gradient 0.0968\n",
      "epoch 622, cost16.1, m gradient -0.0196, b gradient 0.0962\n",
      "epoch 623, cost16.1, m gradient -0.0195, b gradient 0.0957\n",
      "epoch 624, cost16.1, m gradient -0.0194, b gradient 0.0951\n",
      "epoch 625, cost16.1, m gradient -0.0192, b gradient 0.0946\n",
      "epoch 626, cost16.1, m gradient -0.0191, b gradient 0.094\n",
      "epoch 627, cost16.1, m gradient -0.019, b gradient 0.0935\n",
      "epoch 628, cost16.1, m gradient -0.0189, b gradient 0.0929\n",
      "epoch 629, cost16.1, m gradient -0.0188, b gradient 0.0924\n",
      "epoch 630, cost16.1, m gradient -0.0187, b gradient 0.0919\n",
      "epoch 631, cost16.1, m gradient -0.0186, b gradient 0.0913\n",
      "epoch 632, cost16.1, m gradient -0.0185, b gradient 0.0908\n",
      "epoch 633, cost16.1, m gradient -0.0184, b gradient 0.0903\n",
      "epoch 634, cost16.1, m gradient -0.0183, b gradient 0.0898\n",
      "epoch 635, cost16.1, m gradient -0.0182, b gradient 0.0892\n",
      "epoch 636, cost16.1, m gradient -0.0181, b gradient 0.0887\n",
      "epoch 637, cost16.1, m gradient -0.0179, b gradient 0.0882\n",
      "epoch 638, cost16.1, m gradient -0.0178, b gradient 0.0877\n",
      "epoch 639, cost16.1, m gradient -0.0177, b gradient 0.0872\n",
      "epoch 640, cost16.1, m gradient -0.0176, b gradient 0.0867\n",
      "epoch 641, cost16.1, m gradient -0.0175, b gradient 0.0862\n",
      "epoch 642, cost16.1, m gradient -0.0174, b gradient 0.0857\n",
      "epoch 643, cost16.1, m gradient -0.0173, b gradient 0.0852\n",
      "epoch 644, cost16.1, m gradient -0.0172, b gradient 0.0847\n",
      "epoch 645, cost16.1, m gradient -0.0171, b gradient 0.0842\n",
      "epoch 646, cost16.1, m gradient -0.017, b gradient 0.0838\n",
      "epoch 647, cost16.1, m gradient -0.0169, b gradient 0.0833\n",
      "epoch 648, cost16.1, m gradient -0.0168, b gradient 0.0828\n",
      "epoch 649, cost16.1, m gradient -0.0167, b gradient 0.0823\n",
      "epoch 650, cost16.1, m gradient -0.0167, b gradient 0.0818\n",
      "epoch 651, cost16.1, m gradient -0.0166, b gradient 0.0814\n",
      "epoch 652, cost16.1, m gradient -0.0164, b gradient 0.0809\n",
      "epoch 653, cost16.1, m gradient -0.0164, b gradient 0.0804\n",
      "epoch 654, cost16.1, m gradient -0.0162, b gradient 0.08\n",
      "epoch 655, cost16.1, m gradient -0.0162, b gradient 0.0795\n",
      "epoch 656, cost16.1, m gradient -0.0161, b gradient 0.079\n",
      "epoch 657, cost16.1, m gradient -0.016, b gradient 0.0786\n",
      "epoch 658, cost16.1, m gradient -0.0159, b gradient 0.0781\n",
      "epoch 659, cost16.1, m gradient -0.0158, b gradient 0.0777\n",
      "epoch 660, cost16.1, m gradient -0.0157, b gradient 0.0772\n",
      "epoch 661, cost16.1, m gradient -0.0156, b gradient 0.0768\n",
      "epoch 662, cost16.1, m gradient -0.0155, b gradient 0.0764\n",
      "epoch 663, cost16.1, m gradient -0.0154, b gradient 0.0759\n",
      "epoch 664, cost16.1, m gradient -0.0153, b gradient 0.0755\n",
      "epoch 665, cost16.1, m gradient -0.0152, b gradient 0.075\n",
      "epoch 666, cost16.1, m gradient -0.0151, b gradient 0.0746\n",
      "epoch 667, cost16.1, m gradient -0.0151, b gradient 0.0742\n",
      "epoch 668, cost16.1, m gradient -0.015, b gradient 0.0738\n",
      "epoch 669, cost16.1, m gradient -0.0149, b gradient 0.0733\n",
      "epoch 670, cost16.1, m gradient -0.0148, b gradient 0.0729\n",
      "epoch 671, cost16.1, m gradient -0.0147, b gradient 0.0725\n",
      "epoch 672, cost16.1, m gradient -0.0147, b gradient 0.0721\n",
      "epoch 673, cost16.1, m gradient -0.0146, b gradient 0.0716\n",
      "epoch 674, cost16.1, m gradient -0.0145, b gradient 0.0712\n",
      "epoch 675, cost16.1, m gradient -0.0144, b gradient 0.0708\n",
      "epoch 676, cost16.1, m gradient -0.0143, b gradient 0.0704\n",
      "epoch 677, cost16.1, m gradient -0.0142, b gradient 0.07\n",
      "epoch 678, cost16.1, m gradient -0.0141, b gradient 0.0696\n",
      "epoch 679, cost16.1, m gradient -0.0141, b gradient 0.0692\n",
      "epoch 680, cost16.1, m gradient -0.014, b gradient 0.0688\n",
      "epoch 681, cost16.1, m gradient -0.0139, b gradient 0.0684\n",
      "epoch 682, cost16.1, m gradient -0.0138, b gradient 0.068\n",
      "epoch 683, cost16.1, m gradient -0.0137, b gradient 0.0676\n",
      "epoch 684, cost16.1, m gradient -0.0137, b gradient 0.0672\n",
      "epoch 685, cost16.1, m gradient -0.0136, b gradient 0.0668\n",
      "epoch 686, cost16.1, m gradient -0.0135, b gradient 0.0665\n",
      "epoch 687, cost16.1, m gradient -0.0134, b gradient 0.0661\n",
      "epoch 688, cost16.1, m gradient -0.0133, b gradient 0.0657\n",
      "epoch 689, cost16.1, m gradient -0.0133, b gradient 0.0653\n",
      "epoch 690, cost16.1, m gradient -0.0132, b gradient 0.0649\n",
      "epoch 691, cost16.1, m gradient -0.0131, b gradient 0.0646\n",
      "epoch 692, cost16.1, m gradient -0.013, b gradient 0.0642\n",
      "epoch 693, cost16.1, m gradient -0.013, b gradient 0.0638\n",
      "epoch 694, cost16.1, m gradient -0.0129, b gradient 0.0635\n",
      "epoch 695, cost16.1, m gradient -0.0128, b gradient 0.0631\n",
      "epoch 696, cost16.1, m gradient -0.0127, b gradient 0.0627\n",
      "epoch 697, cost16.1, m gradient -0.0127, b gradient 0.0624\n",
      "epoch 698, cost16.1, m gradient -0.0126, b gradient 0.062\n",
      "epoch 699, cost16.1, m gradient -0.0125, b gradient 0.0616\n",
      "epoch 700, cost16.1, m gradient -0.0125, b gradient 0.0613\n",
      "epoch 701, cost16.1, m gradient -0.0124, b gradient 0.0609\n",
      "epoch 702, cost16.1, m gradient -0.0123, b gradient 0.0606\n",
      "epoch 703, cost16.1, m gradient -0.0122, b gradient 0.0602\n",
      "epoch 704, cost16.1, m gradient -0.0122, b gradient 0.0599\n",
      "epoch 705, cost16.1, m gradient -0.0121, b gradient 0.0595\n",
      "epoch 706, cost16.1, m gradient -0.012, b gradient 0.0592\n",
      "epoch 707, cost16.1, m gradient -0.012, b gradient 0.0589\n",
      "epoch 708, cost16.1, m gradient -0.0119, b gradient 0.0585\n",
      "epoch 709, cost16.1, m gradient -0.0118, b gradient 0.0582\n",
      "epoch 710, cost16.1, m gradient -0.0117, b gradient 0.0579\n",
      "epoch 711, cost16.1, m gradient -0.0117, b gradient 0.0575\n",
      "epoch 712, cost16.1, m gradient -0.0116, b gradient 0.0572\n",
      "epoch 713, cost16.1, m gradient -0.0115, b gradient 0.0569\n",
      "epoch 714, cost16.1, m gradient -0.0115, b gradient 0.0565\n",
      "epoch 715, cost16.1, m gradient -0.0114, b gradient 0.0562\n",
      "epoch 716, cost16.1, m gradient -0.0114, b gradient 0.0559\n",
      "epoch 717, cost16.1, m gradient -0.0113, b gradient 0.0556\n",
      "epoch 718, cost16.1, m gradient -0.0112, b gradient 0.0552\n",
      "epoch 719, cost16.1, m gradient -0.0112, b gradient 0.0549\n",
      "epoch 720, cost16.1, m gradient -0.0111, b gradient 0.0546\n",
      "epoch 721, cost16.1, m gradient -0.011, b gradient 0.0543\n",
      "epoch 722, cost16.1, m gradient -0.0109, b gradient 0.054\n",
      "epoch 723, cost16.1, m gradient -0.0109, b gradient 0.0537\n",
      "epoch 724, cost16.1, m gradient -0.0108, b gradient 0.0533\n",
      "epoch 725, cost16.1, m gradient -0.0108, b gradient 0.053\n",
      "epoch 726, cost16.1, m gradient -0.0107, b gradient 0.0527\n",
      "epoch 727, cost16.1, m gradient -0.0107, b gradient 0.0524\n",
      "epoch 728, cost16.1, m gradient -0.0106, b gradient 0.0521\n",
      "epoch 729, cost16.1, m gradient -0.0106, b gradient 0.0518\n",
      "epoch 730, cost16.1, m gradient -0.0105, b gradient 0.0515\n",
      "epoch 731, cost16.1, m gradient -0.0104, b gradient 0.0512\n",
      "epoch 732, cost16.1, m gradient -0.0103, b gradient 0.0509\n",
      "epoch 733, cost16.1, m gradient -0.0103, b gradient 0.0506\n",
      "epoch 734, cost16.1, m gradient -0.0102, b gradient 0.0504\n",
      "epoch 735, cost16.1, m gradient -0.0102, b gradient 0.0501\n",
      "epoch 736, cost16.1, m gradient -0.0101, b gradient 0.0498\n",
      "epoch 737, cost16.1, m gradient -0.0101, b gradient 0.0495\n",
      "epoch 738, cost16.1, m gradient -0.01, b gradient 0.0492\n",
      "epoch 739, cost16.1, m gradient -0.00995, b gradient 0.0489\n",
      "epoch 740, cost16.1, m gradient -0.00991, b gradient 0.0486\n",
      "epoch 741, cost16.1, m gradient -0.00983, b gradient 0.0484\n",
      "epoch 742, cost16.1, m gradient -0.00978, b gradient 0.0481\n",
      "epoch 743, cost16.1, m gradient -0.00971, b gradient 0.0478\n",
      "epoch 744, cost16.1, m gradient -0.00964, b gradient 0.0475\n",
      "epoch 745, cost16.1, m gradient -0.0096, b gradient 0.0472\n",
      "epoch 746, cost16.1, m gradient -0.00955, b gradient 0.047\n",
      "epoch 747, cost16.1, m gradient -0.00949, b gradient 0.0467\n",
      "epoch 748, cost16.1, m gradient -0.00942, b gradient 0.0464\n",
      "epoch 749, cost16.1, m gradient -0.00936, b gradient 0.0462\n",
      "epoch 750, cost16.1, m gradient -0.00933, b gradient 0.0459\n",
      "epoch 751, cost16.1, m gradient -0.00927, b gradient 0.0456\n",
      "epoch 752, cost16.1, m gradient -0.00923, b gradient 0.0454\n",
      "epoch 753, cost16.1, m gradient -0.00917, b gradient 0.0451\n",
      "epoch 754, cost16.1, m gradient -0.00912, b gradient 0.0449\n",
      "epoch 755, cost16.1, m gradient -0.00906, b gradient 0.0446\n",
      "epoch 756, cost16.1, m gradient -0.00901, b gradient 0.0443\n",
      "epoch 757, cost16.1, m gradient -0.00897, b gradient 0.0441\n",
      "epoch 758, cost16.1, m gradient -0.00893, b gradient 0.0438\n",
      "epoch 759, cost16.1, m gradient -0.00884, b gradient 0.0436\n",
      "epoch 760, cost16.1, m gradient -0.0088, b gradient 0.0433\n",
      "epoch 761, cost16.1, m gradient -0.00875, b gradient 0.0431\n",
      "epoch 762, cost16.1, m gradient -0.00869, b gradient 0.0428\n",
      "epoch 763, cost16.1, m gradient -0.00867, b gradient 0.0426\n",
      "epoch 764, cost16.1, m gradient -0.00861, b gradient 0.0423\n",
      "epoch 765, cost16.1, m gradient -0.00856, b gradient 0.0421\n",
      "epoch 766, cost16.1, m gradient -0.00851, b gradient 0.0418\n",
      "epoch 767, cost16.1, m gradient -0.00846, b gradient 0.0416\n",
      "epoch 768, cost16.1, m gradient -0.0084, b gradient 0.0414\n",
      "epoch 769, cost16.1, m gradient -0.00837, b gradient 0.0411\n",
      "epoch 770, cost16.1, m gradient -0.00831, b gradient 0.0409\n",
      "epoch 771, cost16.1, m gradient -0.00828, b gradient 0.0407\n",
      "epoch 772, cost16.1, m gradient -0.00821, b gradient 0.0404\n",
      "epoch 773, cost16.1, m gradient -0.00816, b gradient 0.0402\n",
      "epoch 774, cost16.1, m gradient -0.00812, b gradient 0.04\n",
      "epoch 775, cost16.1, m gradient -0.00807, b gradient 0.0397\n",
      "epoch 776, cost16.1, m gradient -0.00801, b gradient 0.0395\n",
      "epoch 777, cost16.1, m gradient -0.00797, b gradient 0.0393\n",
      "epoch 778, cost16.1, m gradient -0.00792, b gradient 0.039\n",
      "epoch 779, cost16.1, m gradient -0.00788, b gradient 0.0388\n",
      "epoch 780, cost16.1, m gradient -0.00783, b gradient 0.0386\n",
      "epoch 781, cost16.1, m gradient -0.00779, b gradient 0.0384\n",
      "epoch 782, cost16.1, m gradient -0.00775, b gradient 0.0381\n",
      "epoch 783, cost16.1, m gradient -0.00772, b gradient 0.0379\n",
      "epoch 784, cost16.1, m gradient -0.00765, b gradient 0.0377\n",
      "epoch 785, cost16.1, m gradient -0.00763, b gradient 0.0375\n",
      "epoch 786, cost16.1, m gradient -0.00758, b gradient 0.0373\n",
      "epoch 787, cost16.1, m gradient -0.00756, b gradient 0.0371\n",
      "epoch 788, cost16.1, m gradient -0.00751, b gradient 0.0368\n",
      "epoch 789, cost16.1, m gradient -0.00746, b gradient 0.0366\n",
      "epoch 790, cost16.1, m gradient -0.00742, b gradient 0.0364\n",
      "epoch 791, cost16.1, m gradient -0.00736, b gradient 0.0362\n",
      "epoch 792, cost16.1, m gradient -0.00733, b gradient 0.036\n",
      "epoch 793, cost16.1, m gradient -0.00728, b gradient 0.0358\n",
      "epoch 794, cost16.1, m gradient -0.00724, b gradient 0.0356\n",
      "epoch 795, cost16.1, m gradient -0.00721, b gradient 0.0354\n",
      "epoch 796, cost16.1, m gradient -0.00713, b gradient 0.0352\n",
      "epoch 797, cost16.1, m gradient -0.00711, b gradient 0.035\n",
      "epoch 798, cost16.1, m gradient -0.00705, b gradient 0.0348\n",
      "epoch 799, cost16.1, m gradient -0.00702, b gradient 0.0346\n",
      "epoch 800, cost16.1, m gradient -0.00696, b gradient 0.0344\n",
      "epoch 801, cost16.1, m gradient -0.00693, b gradient 0.0342\n",
      "epoch 802, cost16.1, m gradient -0.00689, b gradient 0.034\n",
      "epoch 803, cost16.1, m gradient -0.00686, b gradient 0.0338\n",
      "epoch 804, cost16.1, m gradient -0.00683, b gradient 0.0336\n",
      "epoch 805, cost16.1, m gradient -0.00678, b gradient 0.0334\n",
      "epoch 806, cost16.1, m gradient -0.00674, b gradient 0.0332\n",
      "epoch 807, cost16.1, m gradient -0.00669, b gradient 0.033\n",
      "epoch 808, cost16.1, m gradient -0.00667, b gradient 0.0328\n",
      "epoch 809, cost16.1, m gradient -0.00663, b gradient 0.0326\n",
      "epoch 810, cost16.1, m gradient -0.00657, b gradient 0.0325\n",
      "epoch 811, cost16.1, m gradient -0.00654, b gradient 0.0323\n",
      "epoch 812, cost16.1, m gradient -0.0065, b gradient 0.0321\n",
      "epoch 813, cost16.1, m gradient -0.00647, b gradient 0.0319\n",
      "epoch 814, cost16.1, m gradient -0.00645, b gradient 0.0317\n",
      "epoch 815, cost16.1, m gradient -0.00639, b gradient 0.0315\n",
      "epoch 816, cost16.1, m gradient -0.00636, b gradient 0.0313\n",
      "epoch 817, cost16.1, m gradient -0.00632, b gradient 0.0312\n",
      "epoch 818, cost16.1, m gradient -0.00629, b gradient 0.031\n",
      "epoch 819, cost16.1, m gradient -0.00626, b gradient 0.0308\n",
      "epoch 820, cost16.1, m gradient -0.00621, b gradient 0.0306\n",
      "epoch 821, cost16.1, m gradient -0.0062, b gradient 0.0304\n",
      "epoch 822, cost16.1, m gradient -0.00614, b gradient 0.0303\n",
      "epoch 823, cost16.1, m gradient -0.00614, b gradient 0.0301\n",
      "epoch 824, cost16.1, m gradient -0.00611, b gradient 0.0299\n",
      "epoch 825, cost16.1, m gradient -0.00606, b gradient 0.0297\n",
      "epoch 826, cost16.1, m gradient -0.00601, b gradient 0.0296\n",
      "epoch 827, cost16.1, m gradient -0.00597, b gradient 0.0294\n",
      "epoch 828, cost16.1, m gradient -0.00593, b gradient 0.0292\n",
      "epoch 829, cost16.1, m gradient -0.0059, b gradient 0.0291\n",
      "epoch 830, cost16.1, m gradient -0.00587, b gradient 0.0289\n",
      "epoch 831, cost16.1, m gradient -0.00582, b gradient 0.0287\n",
      "epoch 832, cost16.1, m gradient -0.00579, b gradient 0.0286\n",
      "epoch 833, cost16.1, m gradient -0.00576, b gradient 0.0284\n",
      "epoch 834, cost16.1, m gradient -0.00575, b gradient 0.0282\n",
      "epoch 835, cost16.1, m gradient -0.00573, b gradient 0.0281\n",
      "epoch 836, cost16.1, m gradient -0.00569, b gradient 0.0279\n",
      "epoch 837, cost16.1, m gradient -0.00564, b gradient 0.0278\n",
      "epoch 838, cost16.1, m gradient -0.0056, b gradient 0.0276\n",
      "epoch 839, cost16.1, m gradient -0.00558, b gradient 0.0274\n",
      "epoch 840, cost16.1, m gradient -0.00553, b gradient 0.0273\n",
      "epoch 841, cost16.1, m gradient -0.00549, b gradient 0.0271\n",
      "epoch 842, cost16.1, m gradient -0.00546, b gradient 0.027\n",
      "epoch 843, cost16.1, m gradient -0.00545, b gradient 0.0268\n",
      "epoch 844, cost16.1, m gradient -0.00541, b gradient 0.0267\n",
      "epoch 845, cost16.1, m gradient -0.00538, b gradient 0.0265\n",
      "epoch 846, cost16.1, m gradient -0.00537, b gradient 0.0263\n",
      "epoch 847, cost16.1, m gradient -0.00535, b gradient 0.0262\n",
      "epoch 848, cost16.1, m gradient -0.0053, b gradient 0.026\n",
      "epoch 849, cost16.1, m gradient -0.00526, b gradient 0.0259\n",
      "epoch 850, cost16.1, m gradient -0.00523, b gradient 0.0257\n",
      "epoch 851, cost16.1, m gradient -0.00521, b gradient 0.0256\n",
      "epoch 852, cost16.1, m gradient -0.00516, b gradient 0.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 853, cost16.1, m gradient -0.00514, b gradient 0.0253\n",
      "epoch 854, cost16.1, m gradient -0.00511, b gradient 0.0252\n",
      "epoch 855, cost16.1, m gradient -0.00506, b gradient 0.025\n",
      "epoch 856, cost16.1, m gradient -0.00506, b gradient 0.0249\n",
      "epoch 857, cost16.1, m gradient -0.00503, b gradient 0.0247\n",
      "epoch 858, cost16.1, m gradient -0.00499, b gradient 0.0246\n",
      "epoch 859, cost16.1, m gradient -0.00497, b gradient 0.0244\n",
      "epoch 860, cost16.1, m gradient -0.00494, b gradient 0.0243\n",
      "epoch 861, cost16.1, m gradient -0.00491, b gradient 0.0242\n",
      "epoch 862, cost16.1, m gradient -0.00488, b gradient 0.024\n",
      "epoch 863, cost16.1, m gradient -0.00486, b gradient 0.0239\n",
      "epoch 864, cost16.1, m gradient -0.00482, b gradient 0.0237\n",
      "epoch 865, cost16.1, m gradient -0.00477, b gradient 0.0236\n",
      "epoch 866, cost16.1, m gradient -0.00476, b gradient 0.0235\n",
      "epoch 867, cost16.1, m gradient -0.00474, b gradient 0.0233\n",
      "epoch 868, cost16.1, m gradient -0.00471, b gradient 0.0232\n",
      "epoch 869, cost16.1, m gradient -0.0047, b gradient 0.0231\n",
      "epoch 870, cost16.1, m gradient -0.00466, b gradient 0.0229\n",
      "epoch 871, cost16.1, m gradient -0.00464, b gradient 0.0228\n",
      "epoch 872, cost16.1, m gradient -0.0046, b gradient 0.0227\n",
      "epoch 873, cost16.1, m gradient -0.00459, b gradient 0.0225\n",
      "epoch 874, cost16.1, m gradient -0.00457, b gradient 0.0224\n",
      "epoch 875, cost16.1, m gradient -0.00453, b gradient 0.0223\n",
      "epoch 876, cost16.1, m gradient -0.00452, b gradient 0.0221\n",
      "epoch 877, cost16.1, m gradient -0.0045, b gradient 0.022\n",
      "epoch 878, cost16.1, m gradient -0.00446, b gradient 0.0219\n",
      "epoch 879, cost16.1, m gradient -0.00444, b gradient 0.0218\n",
      "epoch 880, cost16.1, m gradient -0.00439, b gradient 0.0216\n",
      "epoch 881, cost16.1, m gradient -0.00437, b gradient 0.0215\n",
      "epoch 882, cost16.1, m gradient -0.00434, b gradient 0.0214\n",
      "epoch 883, cost16.1, m gradient -0.00434, b gradient 0.0213\n",
      "epoch 884, cost16.1, m gradient -0.00433, b gradient 0.0211\n",
      "epoch 885, cost16.1, m gradient -0.00429, b gradient 0.021\n",
      "epoch 886, cost16.1, m gradient -0.00426, b gradient 0.0209\n",
      "epoch 887, cost16.1, m gradient -0.00423, b gradient 0.0208\n",
      "epoch 888, cost16.1, m gradient -0.00422, b gradient 0.0207\n",
      "epoch 889, cost16.1, m gradient -0.0042, b gradient 0.0205\n",
      "epoch 890, cost16.1, m gradient -0.00417, b gradient 0.0204\n",
      "epoch 891, cost16.1, m gradient -0.00412, b gradient 0.0203\n",
      "epoch 892, cost16.1, m gradient -0.0041, b gradient 0.0202\n",
      "epoch 893, cost16.1, m gradient -0.00408, b gradient 0.0201\n",
      "epoch 894, cost16.1, m gradient -0.00406, b gradient 0.02\n",
      "epoch 895, cost16.1, m gradient -0.00402, b gradient 0.0198\n",
      "epoch 896, cost16.1, m gradient -0.004, b gradient 0.0197\n",
      "epoch 897, cost16.1, m gradient -0.00399, b gradient 0.0196\n",
      "epoch 898, cost16.1, m gradient -0.00396, b gradient 0.0195\n",
      "epoch 899, cost16.1, m gradient -0.00396, b gradient 0.0194\n",
      "epoch 900, cost16.1, m gradient -0.00392, b gradient 0.0193\n",
      "epoch 901, cost16.1, m gradient -0.00389, b gradient 0.0192\n",
      "epoch 902, cost16.1, m gradient -0.00386, b gradient 0.0191\n",
      "epoch 903, cost16.1, m gradient -0.00386, b gradient 0.0189\n",
      "epoch 904, cost16.1, m gradient -0.00386, b gradient 0.0188\n",
      "epoch 905, cost16.1, m gradient -0.00383, b gradient 0.0187\n",
      "epoch 906, cost16.1, m gradient -0.00381, b gradient 0.0186\n",
      "epoch 907, cost16.1, m gradient -0.00378, b gradient 0.0185\n",
      "epoch 908, cost16.1, m gradient -0.00375, b gradient 0.0184\n",
      "epoch 909, cost16.1, m gradient -0.00373, b gradient 0.0183\n",
      "epoch 910, cost16.1, m gradient -0.00371, b gradient 0.0182\n",
      "epoch 911, cost16.1, m gradient -0.00369, b gradient 0.0181\n",
      "epoch 912, cost16.1, m gradient -0.00364, b gradient 0.018\n",
      "epoch 913, cost16.1, m gradient -0.00363, b gradient 0.0179\n",
      "epoch 914, cost16.1, m gradient -0.00362, b gradient 0.0178\n",
      "epoch 915, cost16.1, m gradient -0.0036, b gradient 0.0177\n",
      "epoch 916, cost16.1, m gradient -0.00356, b gradient 0.0176\n",
      "epoch 917, cost16.1, m gradient -0.00356, b gradient 0.0175\n",
      "epoch 918, cost16.1, m gradient -0.00355, b gradient 0.0174\n",
      "epoch 919, cost16.1, m gradient -0.00352, b gradient 0.0173\n",
      "epoch 920, cost16.1, m gradient -0.00351, b gradient 0.0172\n",
      "epoch 921, cost16.1, m gradient -0.00348, b gradient 0.0171\n",
      "epoch 922, cost16.1, m gradient -0.00346, b gradient 0.017\n",
      "epoch 923, cost16.1, m gradient -0.00346, b gradient 0.0169\n",
      "epoch 924, cost16.1, m gradient -0.00343, b gradient 0.0168\n",
      "epoch 925, cost16.1, m gradient -0.0034, b gradient 0.0167\n",
      "epoch 926, cost16.1, m gradient -0.00338, b gradient 0.0166\n",
      "epoch 927, cost16.1, m gradient -0.00336, b gradient 0.0165\n",
      "epoch 928, cost16.1, m gradient -0.00335, b gradient 0.0164\n",
      "epoch 929, cost16.1, m gradient -0.00333, b gradient 0.0163\n",
      "epoch 930, cost16.1, m gradient -0.0033, b gradient 0.0162\n",
      "epoch 931, cost16.1, m gradient -0.00327, b gradient 0.0161\n",
      "epoch 932, cost16.1, m gradient -0.00326, b gradient 0.016\n",
      "epoch 933, cost16.1, m gradient -0.00324, b gradient 0.0159\n",
      "epoch 934, cost16.1, m gradient -0.00323, b gradient 0.0158\n",
      "epoch 935, cost16.1, m gradient -0.00319, b gradient 0.0158\n",
      "epoch 936, cost16.1, m gradient -0.00317, b gradient 0.0157\n",
      "epoch 937, cost16.1, m gradient -0.00316, b gradient 0.0156\n",
      "epoch 938, cost16.1, m gradient -0.00315, b gradient 0.0155\n",
      "epoch 939, cost16.1, m gradient -0.00313, b gradient 0.0154\n",
      "epoch 940, cost16.1, m gradient -0.00312, b gradient 0.0153\n",
      "epoch 941, cost16.1, m gradient -0.00308, b gradient 0.0152\n",
      "epoch 942, cost16.1, m gradient -0.00308, b gradient 0.0151\n",
      "epoch 943, cost16.1, m gradient -0.00306, b gradient 0.015\n",
      "epoch 944, cost16.1, m gradient -0.00304, b gradient 0.015\n",
      "epoch 945, cost16.1, m gradient -0.00303, b gradient 0.0149\n",
      "epoch 946, cost16.1, m gradient -0.00301, b gradient 0.0148\n",
      "epoch 947, cost16.1, m gradient -0.00297, b gradient 0.0147\n",
      "epoch 948, cost16.1, m gradient -0.00296, b gradient 0.0146\n",
      "epoch 949, cost16.1, m gradient -0.00295, b gradient 0.0145\n",
      "epoch 950, cost16.1, m gradient -0.00293, b gradient 0.0144\n",
      "epoch 951, cost16.1, m gradient -0.0029, b gradient 0.0144\n",
      "epoch 952, cost16.1, m gradient -0.00292, b gradient 0.0143\n",
      "epoch 953, cost16.1, m gradient -0.00287, b gradient 0.0142\n",
      "epoch 954, cost16.1, m gradient -0.00286, b gradient 0.0141\n",
      "epoch 955, cost16.1, m gradient -0.00286, b gradient 0.014\n",
      "epoch 956, cost16.1, m gradient -0.00283, b gradient 0.0139\n",
      "epoch 957, cost16.1, m gradient -0.00282, b gradient 0.0139\n",
      "epoch 958, cost16.1, m gradient -0.00278, b gradient 0.0138\n",
      "epoch 959, cost16.1, m gradient -0.00277, b gradient 0.0137\n",
      "epoch 960, cost16.1, m gradient -0.00277, b gradient 0.0136\n",
      "epoch 961, cost16.1, m gradient -0.00276, b gradient 0.0135\n",
      "epoch 962, cost16.1, m gradient -0.00274, b gradient 0.0135\n",
      "epoch 963, cost16.1, m gradient -0.00271, b gradient 0.0134\n",
      "epoch 964, cost16.1, m gradient -0.00271, b gradient 0.0133\n",
      "epoch 965, cost16.1, m gradient -0.00271, b gradient 0.0132\n",
      "epoch 966, cost16.1, m gradient -0.00271, b gradient 0.0132\n",
      "epoch 967, cost16.1, m gradient -0.00269, b gradient 0.0131\n",
      "epoch 968, cost16.1, m gradient -0.00267, b gradient 0.013\n",
      "epoch 969, cost16.1, m gradient -0.00265, b gradient 0.0129\n",
      "epoch 970, cost16.1, m gradient -0.00262, b gradient 0.0129\n",
      "epoch 971, cost16.1, m gradient -0.00262, b gradient 0.0128\n",
      "epoch 972, cost16.1, m gradient -0.00263, b gradient 0.0127\n",
      "epoch 973, cost16.1, m gradient -0.00257, b gradient 0.0126\n",
      "epoch 974, cost16.1, m gradient -0.00255, b gradient 0.0126\n",
      "epoch 975, cost16.1, m gradient -0.00253, b gradient 0.0125\n",
      "epoch 976, cost16.1, m gradient -0.00255, b gradient 0.0124\n",
      "epoch 977, cost16.1, m gradient -0.0025, b gradient 0.0124\n",
      "epoch 978, cost16.1, m gradient -0.00251, b gradient 0.0123\n",
      "epoch 979, cost16.1, m gradient -0.0025, b gradient 0.0122\n",
      "epoch 980, cost16.1, m gradient -0.00248, b gradient 0.0121\n",
      "epoch 981, cost16.1, m gradient -0.00247, b gradient 0.0121\n",
      "epoch 982, cost16.1, m gradient -0.00244, b gradient 0.012\n",
      "epoch 983, cost16.1, m gradient -0.00242, b gradient 0.0119\n",
      "epoch 984, cost16.1, m gradient -0.00241, b gradient 0.0119\n",
      "epoch 985, cost16.1, m gradient -0.00241, b gradient 0.0118\n",
      "epoch 986, cost16.1, m gradient -0.00239, b gradient 0.0117\n",
      "epoch 987, cost16.1, m gradient -0.00239, b gradient 0.0117\n",
      "epoch 988, cost16.1, m gradient -0.00238, b gradient 0.0116\n",
      "epoch 989, cost16.1, m gradient -0.00235, b gradient 0.0115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 990, cost16.1, m gradient -0.00231, b gradient 0.0115\n",
      "epoch 991, cost16.1, m gradient -0.00231, b gradient 0.0114\n",
      "epoch 992, cost16.1, m gradient -0.00231, b gradient 0.0113\n",
      "epoch 993, cost16.1, m gradient -0.00231, b gradient 0.0113\n",
      "epoch 994, cost16.1, m gradient -0.00229, b gradient 0.0112\n",
      "epoch 995, cost16.1, m gradient -0.00228, b gradient 0.0111\n",
      "epoch 996, cost16.1, m gradient -0.00226, b gradient 0.0111\n",
      "epoch 997, cost16.1, m gradient -0.00223, b gradient 0.011\n",
      "epoch 998, cost16.1, m gradient -0.00224, b gradient 0.0109\n",
      "epoch 999, cost16.1, m gradient -0.00223, b gradient 0.0109\n"
     ]
    }
   ],
   "source": [
    "# iterate the optimizer \n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()    # reset to prevent memory leak\n",
    "    yhat = regression(x, m, b)\n",
    "    cost = cost_func(yhat,y) # calculate gradient of cost wrt to m & b\n",
    "    cost.backward()          # autodiff to trace \n",
    "    optimizer.step()         # gradient descent to overwrite m and b\n",
    "    print('epoch {}, cost{}, m gradient {}, b gradient {}'.format(epoch, '%.3g'%cost.item(), '%.3g'%m.grad.item(), '%.3g'%b.grad.item()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "regression_plot(x,y,m,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.8268], requires_grad=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.6150], requires_grad=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 80 Calculating Partial Derivatives with AutoDiff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate\n",
    "- the value of z\n",
    "- the slope of z with respect to x\n",
    "- the slope of z with respect to y\n",
    "\n",
    "at the points where\n",
    "1. x = 3, y = 0\n",
    "2. x = 2, y = 3\n",
    "3. x =-2, y = -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(my_x, my_y):\n",
    "    return my_x**2 - my_y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(my_x, my_y):\n",
    "    x = torch.tensor(my_x).requires_grad_()\n",
    "    y = torch.tensor(my_y).requires_grad_()\n",
    "    z = f(x,y)\n",
    "    z.backward()\n",
    "    \n",
    "    output1 = z\n",
    "    output2 = x.grad\n",
    "    output3 = y.grad\n",
    "    \n",
    "    print ('z = ', output1)\n",
    "    print('slope of z wrt x = ', output2)\n",
    "    print('slope of z wrt y = ', output3)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor(9., grad_fn=<SubBackward0>)\n",
      "slope of z wrt x =  tensor(6.)\n",
      "slope of z wrt y =  tensor(-0.)\n"
     ]
    }
   ],
   "source": [
    "f2(3.0,0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor(-5., grad_fn=<SubBackward0>)\n",
      "slope of z wrt x =  tensor(4.)\n",
      "slope of z wrt y =  tensor(-6.)\n"
     ]
    }
   ],
   "source": [
    "f2(2.0, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z =  tensor(-5., grad_fn=<SubBackward0>)\n",
      "slope of z wrt x =  tensor(-4.)\n",
      "slope of z wrt y =  tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "f2(-2.0, -3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 102 exercise in numerical integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.0, 7.771561172376096e-14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.integrate import quad\n",
    "\n",
    "def f(x):\n",
    "    return 2*x\n",
    "\n",
    "quad(f,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 112 exercise on event probability"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create a python method that solves exercise 3 and incorporates (n k) formula. calculate the probability -- in five tosses -- throwing each of 1, 2, 3, 4, 5 heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factorial(n):\n",
    "    pdt = 1\n",
    "    for i in range (1, n+1):\n",
    "        pdt = pdt * i\n",
    "    return pdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob (n,k):\n",
    "    nk = factorial(n)/ (factorial(k)*factorial(n-k))\n",
    "    return nk/2**n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03125, 0.15625, 0.3125, 0.3125, 0.15625, 0.03125]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[prob(5, i) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
